{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9453cc0-706c-47a0-8a4c-a467961df56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import time\n",
    "import gget\n",
    "import scipy\n",
    "from scipy.sparse import csr_matrix\n",
    "import anndata as an\n",
    "import scanpy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import random\n",
    "from importlib import reload\n",
    "import warnings\n",
    "import ot\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "import surprise as sup\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\"\"\"WARNING: no warnings\"\"\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# local imports\n",
    "import anndata_utils as anntools\n",
    "\n",
    "source_path = os.path.abspath(\"../source/\")\n",
    "sys.path.append(source_path)\n",
    "import centrality as central\n",
    "import matrix\n",
    "import utils as ut\n",
    "import plotting as plt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fe13c94-97da-45c8-87ea-92e35c79e7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/scratch/indikar_root/indikar1/shared_data/higher_order/by_chromosome/population_mESC_1000000_chr2.h5ad', '/scratch/indikar_root/indikar1/shared_data/higher_order/by_chromosome/singlecell_mESC_1000000_chr2.h5ad']\n",
      "\n",
      "population_path='/scratch/indikar_root/indikar1/shared_data/higher_order/by_chromosome/population_mESC_1000000_chr2.h5ad'\n",
      "singlecell_path='/scratch/indikar_root/indikar1/shared_data/higher_order/by_chromosome/singlecell_mESC_1000000_chr2.h5ad'\n"
     ]
    }
   ],
   "source": [
    "resolution = 1000000\n",
    "chrom = \"chr2\"\n",
    "\n",
    "dpath = \"/scratch/indikar_root/indikar1/shared_data/higher_order/by_chromosome/\"\n",
    "\n",
    "file_list = sorted(glob.glob(f\"{dpath}*_{resolution}_{chrom}*\"))\n",
    "print(file_list)\n",
    "\n",
    "population_path = file_list[0]\n",
    "singlecell_path = file_list[1]\n",
    "\n",
    "print()\n",
    "\n",
    "print(f\"{population_path=}\")\n",
    "print(f\"{singlecell_path=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea4c281-4b5c-4957-92d6-c93c245fe585",
   "metadata": {},
   "source": [
    "# load population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc0b6d8-d917-4902-9328-7ee9e9817004",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()  # Record the start time\n",
    "adata = sc.read_h5ad(population_path)\n",
    "end_time = time.time()  # Record the end time\n",
    "print(f\"Time taken to read the file: {end_time - start_time:.2f} seconds\")\n",
    "sc.logging.print_memory_usage()\n",
    "adata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5daf3c-2087-42c5-836d-fff80a223ad3",
   "metadata": {},
   "source": [
    "# load single-cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cff13b-d348-4c5c-8f60-daaa71259195",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()  # Record the start time\n",
    "bdata = sc.read_h5ad(singlecell_path)\n",
    "end_time = time.time()  # Record the end time\n",
    "print(f\"Time taken to read the file: {end_time - start_time:.2f} seconds\")\n",
    "sc.logging.print_memory_usage()\n",
    "bdata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f50376-996b-4a0b-9e8c-50c040a20378",
   "metadata": {},
   "source": [
    "# QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1ee553-a344-42c1-a9e9-a8ddc053530a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_outliers_iqr(df_column):\n",
    "  \"\"\"\n",
    "  Identifies outliers in a pandas DataFrame column using the IQR method.\n",
    "\n",
    "  Args:\n",
    "    df_column: A pandas Series representing the column to analyze.\n",
    "\n",
    "  Returns:\n",
    "    A boolean mask with True for outliers and False otherwise.\n",
    "  \"\"\"\n",
    "  Q1 = df_column.quantile(0.15)\n",
    "  Q3 = df_column.quantile(0.85)\n",
    "  IQR = Q3 - Q1\n",
    "  lower_bound = Q1 - 1.5 * IQR\n",
    "  upper_bound = Q3 + 1.5 * IQR\n",
    "  return (df_column < lower_bound) | (df_column > upper_bound)\n",
    "\n",
    "adata.obs['degree_outlier'] = find_outliers_iqr(adata.obs['chrom_degree'])\n",
    "\n",
    "adata.obs[adata.obs['degree_outlier']][['chrom_bin', 'chrom_degree', 'degree_outlier']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4000620a-741f-434e-9522-d23884975e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers\n",
    "remove_bins = adata.obs[adata.obs['degree_outlier']].index.to_list()\n",
    "print(f\"Removing top {len(remove_bins)} outlier loci: \")\n",
    "print(remove_bins)\n",
    "\n",
    "adata = adata[~adata.obs_names.isin(remove_bins), :].copy()\n",
    "bdata = bdata[~bdata.obs_names.isin(remove_bins), :].copy()\n",
    "\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea59f3c-2f47-4a09-8a4b-d34cebac35bc",
   "metadata": {},
   "source": [
    "# Clique-expand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe936b5-af1e-4d73-aabd-4266cf8f40d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix.expand_and_normalize_anndata(adata, oe_kr=True)\n",
    "print()\n",
    "matrix.expand_and_normalize_anndata(bdata, oe_kr=True)\n",
    "\n",
    "adata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef05971-eebd-41cb-850d-fb68484e2706",
   "metadata": {},
   "source": [
    "# Centrality measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c7a685-598e-423a-b58b-8c289658ac94",
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_centralities = {\n",
    "    'ce_eigenvector_centrality' : {\n",
    "        'function' : nx.eigenvector_centrality,\n",
    "        'weight' : True\n",
    "    },\n",
    "    'ce_betweenness_centrality' : {\n",
    "        'function' : nx.betweenness_centrality,\n",
    "        'weight' : True\n",
    "    },\n",
    "    'ce_pagerank' : {\n",
    "        'function' : nx.pagerank,\n",
    "        'weight' : True\n",
    "    },\n",
    "}\n",
    "\n",
    "obsm_key = 'A_oe'\n",
    "A = adata.obsm[obsm_key].copy()\n",
    "# A = A.mask(np.eye(A.shape[0], dtype=bool), 0)\n",
    "\n",
    "G = nx.from_pandas_adjacency(A)\n",
    "print(G)\n",
    "\n",
    "for label, d in ce_centralities.items():\n",
    "    if d['weight']:\n",
    "        centrality = d['function'](G, weight='weight')\n",
    "    else:\n",
    "        centrality = d['function'](G)\n",
    "        \n",
    "    adata.obs[label] = adata.obs.index.map(centrality)\n",
    "    adata.obs[label] = ut.min_max(adata.obs[label])\n",
    "\n",
    "adata.obs[list(ce_centralities.keys())].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81afe877-4e14-45b7-88a6-b150e02a9de9",
   "metadata": {},
   "source": [
    "# higher-order centralities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095a5387-4884-4731-bd33-8c23adce6a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_incidence_matrix(matrix, reg=0.85):\n",
    "    \"\"\"Balances a sparse matrix to be doubly stochastic using Sinkhorn-Knopp.\n",
    "\n",
    "    Args:\n",
    "        matrix: A scipy.sparse matrix.\n",
    "        reg: Regularization parameter for Sinkhorn-Knopp algorithm.\n",
    "\n",
    "    Returns:\n",
    "        A balanced sparse matrix.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    matrix = matrix.toarray()  # Convert to dense array for POT\n",
    "    a = np.ones(matrix.shape[0]) / matrix.shape[0]  # Uniform row distribution\n",
    "    b = np.ones(matrix.shape[1]) / matrix.shape[1]  # Uniform column distribution\n",
    "    balanced_matrix = ot.sinkhorn(a, b, matrix, reg)  # Use input regularization parameter\n",
    "    end_time = time.time()\n",
    "    print(f\"Balancing matrix took: {end_time - start_time:.2f} seconds\")\n",
    "    return csr_matrix(balanced_matrix)  # Convert back to sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399f68cf-cd7b-42b9-ae55-e7b11eeb19f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the principal singular value of the incidence matrix\n",
    "H = adata.to_df().copy()\n",
    "print(f\"Raw: {H.shape=}\")\n",
    "H = H.T.drop_duplicates().T\n",
    "print(f\"De-duped: {H.shape=}\")\n",
    "\n",
    "node_weight_attr = 'ATACSeq_1' # must be an obs column\n",
    "node_weights = adata.obs.loc[H.index, node_weight_attr].values\n",
    "\n",
    "\n",
    "svd = TruncatedSVD(n_components=1, n_iter=10)\n",
    "adata.obs['singular_vector_1'] = ut.min_max(svd.fit_transform(H))\n",
    "\n",
    "# hypergraph centralities\n",
    "hge_functions = {\n",
    "    'hge_logexp_unweighted' : {\n",
    "        'function' : 'log-exp',\n",
    "        'weights' : None,\n",
    "    },\n",
    "    'hge_logexp_degree_weighted' : {\n",
    "        'function' : 'log-exp',\n",
    "        'weights' : 1 / (H.sum(axis=1).values + 1),\n",
    "    },\n",
    "    'hge_logexp_RNA_weighted' : {\n",
    "        'function' : 'log-exp',\n",
    "        'weights' : 1 / (adata.obs.loc[H.index, 'RNA_2'].values + 1)\n",
    "    },\n",
    "    'hge_logexp_ATAC_weighted' : {\n",
    "        'function' : 'log-exp',\n",
    "        'weights' : 1 / (adata.obs.loc[H.index, 'ATACSeq_1'].values + 1)\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "hge_centralities = []\n",
    "\n",
    "for label, d in hge_functions.items():\n",
    "    start_time = time.time()  # Record start time\n",
    "    node, edge = central.nonlinear_eigenvector_centrality(\n",
    "        H,\n",
    "        function=d['function'],\n",
    "        node_weights=d['weights'],\n",
    "    )\n",
    "\n",
    "    hge_centralities.append(label)\n",
    "    adata.obs[label] = ut.min_max(node)\n",
    "\n",
    "    end_time = time.time()  # Record end time\n",
    "    print(f\"{label} calculation took: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5fb69f-6723-441e-97f0-a324f717f096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# umap_features = [    \n",
    "#     'ATACSeq_3',\n",
    "#     'CTCF',\n",
    "#     'H3K27ac',\n",
    "#     'H3K27me3', \n",
    "#     'RNA_2', \n",
    "#     'ce_eigenvector_centrality',\n",
    "#     'singular_vector_1',\n",
    "#     'hge_logexp_unweighted',\n",
    "#     'hge_logexp_RNA_weighted',\n",
    "#     'hge_logexp_ATAC_weighted',\n",
    "# ]\n",
    "\n",
    "# adata.obsm['X_features'] = adata.obs[umap_features].copy()\n",
    "\n",
    "# sc.pp.neighbors(\n",
    "#     adata, \n",
    "#     use_rep='X_features',\n",
    "#     n_neighbors=5,\n",
    "# )\n",
    "\n",
    "# sc.tl.umap(\n",
    "#     adata,\n",
    "# )\n",
    "\n",
    "# adata.obs['UMAP 1'] = adata.obsm['X_umap'][:, 0]\n",
    "# adata.obs['UMAP 2'] = adata.obsm['X_umap'][:, 1]\n",
    "\n",
    "# plt.rcParams['figure.dpi'] = 200\n",
    "# plt.rcParams['figure.figsize'] = 3, 3\n",
    "\n",
    "# sns.scatterplot(\n",
    "#     data=adata.obs,\n",
    "#     x='UMAP 1',\n",
    "#     y='UMAP 2',\n",
    "#     ec='none',\n",
    "# )\n",
    "\n",
    "# plt.yticks([])\n",
    "# plt.xticks([])\n",
    "# sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b3455f-285d-4fbb-a941-b3817a1ec8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.rcParams['figure.dpi'] = 200\n",
    "# plt.rcParams['figure.figsize'] = 4, 4\n",
    "\n",
    "# cmap = 'hot_r'\n",
    "# colorbars = False\n",
    "\n",
    "# for feature in umap_features:\n",
    "#     sns.scatterplot(\n",
    "#         data=adata.obs,\n",
    "#         x='UMAP 1',\n",
    "#         y='UMAP 2',\n",
    "#         ec='k',\n",
    "#         s=35,\n",
    "#         hue=feature,\n",
    "#         palette=cmap,\n",
    "#         legend=False,\n",
    "#     )\n",
    "\n",
    "#     plt.yticks([])\n",
    "#     plt.xticks([])\n",
    "#     plt.title(feature)\n",
    "#     sns.despine()\n",
    "#     plt.show()\n",
    "    \n",
    "#     if colorbars:\n",
    "#         plt2.make_colorbar(cmap=cmap, tick_labels=['Low', 'High'])\n",
    "#         plt.show()\n",
    "\n",
    "#     # break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a99b76-dd46-4950-9a00-93aacc0d0469",
   "metadata": {},
   "source": [
    "# Extract the core from population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e49b99-c70b-42b2-93ee-6a80ada17a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "core_score = 'hge_logexp_RNA_weighted'\n",
    "core_threshold_quantile = 0.75\n",
    "order_threshold = 2\n",
    "\n",
    "vector = adata.obs[core_score].values\n",
    "threshold = np.quantile(vector, core_threshold_quantile)\n",
    "core_nodes = adata.obs[adata.obs[core_score] > threshold].index.to_list()\n",
    "\n",
    "# extract the core from population\n",
    "core = adata[core_nodes, :].copy()\n",
    "core = core[:, core.X.sum(axis=0) > order_threshold].copy()\n",
    "\n",
    "H_core = core.to_df()\n",
    "print(f\"{H_core.shape=}\")\n",
    "H_core.columns = [f\"core_{x}\" for x in H_core.columns]\n",
    "H_core.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac1af40-9eb7-4f75-84b2-644e4e6f2bfc",
   "metadata": {},
   "source": [
    "# create a set of single-cell incidence matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a889bce7-d5f3-455c-90a3-634be4db5918",
   "metadata": {},
   "outputs": [],
   "source": [
    "incidence_matrices = {}\n",
    "cell_ids = bdata.var['basename'].unique()\n",
    "num_cells = len(cell_ids)\n",
    "total_time = 0.0  # Initialize cumulative time\n",
    "\n",
    "for i, cell_id in enumerate(cell_ids):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # extract the single-cell\n",
    "    sc_data = bdata[:, bdata.var['basename'] == cell_id].copy()\n",
    "    H_o = sc_data.to_df()\n",
    "    H_o = H_o.T.drop_duplicates().T  # Transpose, drop duplicates, transpose back\n",
    "    H_o.columns = [f\"{cell_id}_{x}\" for x in H_o.columns]\n",
    "\n",
    "    incidence_matrices[cell_id] = H_o\n",
    "\n",
    "    # Update timing information\n",
    "    elapsed_time = time.time() - start_time\n",
    "    total_time += elapsed_time\n",
    "\n",
    "    # Periodic status updates\n",
    "    if (i+1) % 50 == 0:\n",
    "        print(f\"Processed {i+1}/{num_cells} cells. \"\n",
    "              f\"Time for last cell: {elapsed_time:.2f} seconds, \"\n",
    "              f\"Cumulative time: {total_time:.2f} seconds\")\n",
    "\n",
    "print(\"Finished processing all cells.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5496c8d4-4b1f-4793-a55e-517a094fd1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b2e63e-fe86-40ab-af1c-3f49a072567c",
   "metadata": {},
   "source": [
    "# Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a86e9a-5b0e-4630-8f33-17d151c41d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "core_dict = adata.obs[core_score].to_dict()\n",
    "n_levels = 5\n",
    "preds = {}\n",
    "\n",
    "start_time = time.time()  # Start the overall timer\n",
    "cumulative_time = 0  # Initialize cumulative time\n",
    "\n",
    "n_cells = 2\n",
    "cell_counter = -1\n",
    "\n",
    "for i, (cell_id, H) in enumerate(incidence_matrices.items()):\n",
    "    cell_counter += 1\n",
    "    if cell_counter == n_cells:\n",
    "        break\n",
    "    loop_start_time = time.time()  # Start the loop timer\n",
    "    print(f\"Processing cell {i+1}/{len(incidence_matrices)} (cell_id: {cell_id})\")\n",
    "\n",
    "    # add the core hyperedges\n",
    "    add_to = adata.to_df().copy()\n",
    "    add_to = add_to.T.drop_duplicates().T\n",
    "    H_imp = pd.concat([H, add_to], axis=1, ignore_index=False)\n",
    "    H_imp = H_imp.fillna(0.0)\n",
    "    print(f\"{H_imp.shape=}\")\n",
    "\n",
    "    # reshape \n",
    "    H_imp = H_imp.reset_index(drop=False)\n",
    "    H_imp = pd.melt(H_imp, id_vars='bin_name')\n",
    "    \n",
    "    H_imp['core_score'] = H_imp['bin_name'].map(core_dict)\n",
    "    H_imp['weighted_value'] = H_imp['core_score'] * H_imp['value']\n",
    "    H_imp['discretized_value'] = (H_imp['weighted_value'] * n_levels).astype(int)\n",
    "    H_imp = H_imp.rename(columns={\n",
    "        'variable': 'userID',\n",
    "        'bin_name': 'itemID',\n",
    "        'discretized_value' : 'rating',\n",
    "    })\n",
    "\n",
    "    reader = sup.Reader(rating_scale=(0, n_levels))\n",
    "    data = sup.Dataset.load_from_df(H_imp[[\"userID\", \"itemID\", \"rating\"]], reader)\n",
    "    trainset = data.build_full_trainset()\n",
    "\n",
    "\n",
    "    sim_options = {\"name\": \"pearson_baseline\"}\n",
    "    \n",
    "    algo = sup.SVDpp(sim_options=sim_options)\n",
    "    algo.fit(trainset)\n",
    "\n",
    "    \"\"\"COULD MAKE UP NEW HYPEREDGES \"\"\"\n",
    "    H_pred = []\n",
    "    for read_name in H.columns.to_list():\n",
    "        for bin_name in H.index.to_list():\n",
    "            pred = algo.predict(uid=read_name, iid=bin_name)\n",
    "            row = {\n",
    "                'read_name': read_name,\n",
    "                'bin_name': bin_name,\n",
    "                'value': pred.est,\n",
    "            }\n",
    "            H_pred.append(row)\n",
    "\n",
    "    H_pred = pd.DataFrame(H_pred)\n",
    "    H_pred = pd.pivot_table(\n",
    "        H_pred,\n",
    "        index='bin_name',\n",
    "        columns='read_name',\n",
    "        values='value',\n",
    "        fill_value=0.0,\n",
    "    )\n",
    "\n",
    "    preds[cell_id] = H_pred\n",
    "\n",
    "    loop_end_time = time.time()  # End the loop timer\n",
    "    cell_time = loop_end_time - loop_start_time\n",
    "    cumulative_time += cell_time  # Update cumulative time\n",
    "    print(f\"Cell processed in {cell_time:.2f} seconds (cumulative: {cumulative_time:.2f} seconds)\")\n",
    "    break\n",
    "\n",
    "end_time = time.time()  # End the overall timer\n",
    "print(f\"Total processing time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fbad2c-3854-4cda-b4b6-60aaed448f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_id = 'o2b67'\n",
    "fig, axs = plt.subplots(1, 2, sharey=True)\n",
    "\n",
    "axs[0].imshow(incidence_matrices[cell_id])\n",
    "axs[1].imshow(preds[cell_id])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa43e1ad-f0ab-484c-8597-c540122d1488",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0403d0b3-89bd-41aa-867c-298700780e37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d2d3ea-760e-42a5-8fbe-c286aa8c79f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# core_dict = adata.obs[core_score].to_dict()\n",
    "\n",
    "# preds = {}\n",
    "\n",
    "\n",
    "# for cell_id, H in incidence_matrices.items():\n",
    "    \n",
    "#     read_names = H.columns.to_list()\n",
    "#     bin_names = H.index.to_list()\n",
    "#     print(f\"{H.shape=}\")\n",
    "\n",
    "#     # append the  core\n",
    "#     H = pd.concat([H, H_core], axis=1, ignore_index=False)\n",
    "#     H = H.fillna(0.0)\n",
    "#     print(f\"{H.shape=}\")\n",
    "\n",
    "#     # develop the dataset from the combined data\n",
    "#     H = H.reset_index(drop=False)\n",
    "#     H = pd.melt(H, id_vars='bin_name')\n",
    "#     H['core_score'] = H['bin_name'].map(core_dict)\n",
    "#     H['rating'] = H['core_score'] * H['value']\n",
    "#     H = H.rename(columns={\n",
    "#         'variable' : 'userID',\n",
    "#         'bin_name' : 'itemID',\n",
    "#     })\n",
    "\n",
    "#     # build the data set\n",
    "#     reader = Reader(rating_scale=(0, 1))\n",
    "#     data = Dataset.load_from_df(H[[\"userID\", \"itemID\", \"rating\"]], reader)\n",
    "#     trainset = data.build_full_trainset()\n",
    "\n",
    "#     # Build an algorithm, and train it.\n",
    "#     algo = SVD()\n",
    "#     algo.fit(trainset)\n",
    "\n",
    "#     # predict only on sc hyperedges\n",
    "#     H_pred = []\n",
    "#     for read_name in read_names:\n",
    "#         for bin_name in bin_names:\n",
    "#             pred = algo.predict(uid=read_name, iid=bin_name)\n",
    "#             row = {\n",
    "#                 'read_name' : read_name,\n",
    "#                 'bin_name' : bin_name,\n",
    "#                 'value' : pred.est,\n",
    "#             }\n",
    "#             H_pred.append(row)\n",
    "\n",
    "#     H_pred = pd.DataFrame(H_pred)\n",
    "#     H_pred = pd.pivot_table(\n",
    "#         H_pred,\n",
    "#         index='bin_name',\n",
    "#         columns='read_name',\n",
    "#         values='value',\n",
    "#         fill_value=0.0,\n",
    "#     )\n",
    "\n",
    "#     print(f\"{H_pred.shape=}\")\n",
    "\n",
    "#     preds[cell_id] = H_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12bcd8e-04be-43bf-adbb-0642ddee13fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ddd17e-26b1-4156-b0d2-b2788ebed792",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacf9637-03ba-4c04-bcf6-406642cc7fd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scanpy",
   "language": "python",
   "name": "scanpy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
